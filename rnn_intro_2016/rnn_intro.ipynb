{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An Introduction to Recurrent Neural Networks\n",
    "\n",
    "Mike Heilman ([@heilman13](https://twitter.com/heilman13))\n",
    "\n",
    "[Civis Analytics](https://civisanalytics.com/) ([@CivisAnalytics](https://twitter.com/CivisAnalytics))\n",
    "\n",
    "November, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Background\n",
    "\n",
    "This is a little intro about RNNs I gave to folks at Civis.  I figured I might as well share it publicly.\n",
    "\n",
    "There are quite a few sources of info on RNNs (links below), but maybe one more variation will help.\n",
    "\n",
    "I'll try to provide some links, but this is by no means a comprehensive literature review!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What to expect\n",
    "\n",
    "We're going to start from very simple models and build up to complex recurrent neural networks.\n",
    "\n",
    "Then, we'll talk about some modeling scenarios and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notation\n",
    "\n",
    "* $X$: input\n",
    "  * shape: (n_examples, n_features)\n",
    "  * aka features, independent variables, predictors\n",
    "* $Y$: outputs\n",
    "  * shape: (n_examples, n_targets)\n",
    "  * aka labels, targets, dependent variables\n",
    "  * I'll assume the targets are discrete and 1-hot encoded here (e.g., words in a vocabulary).\n",
    "* $W$: the parameters for the model\n",
    "  * a.k.a. weights, coefficients\n",
    "  * This is usually many matrices. I'll use subscripts when needed.\n",
    "  * typically, shape: (layer_input_size, layer_output_size)\n",
    "  * I'll also use $U$ later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "$ Y = XW $\n",
    "\n",
    "<img src=\"nn_diagrams_logistic.svg\" width=\"100px\"/>\n",
    "\n",
    "### Notes\n",
    "\n",
    "* There's often an intercept/bias term $b$ added at the end of the above eq., but I'm leaving it out for simplicity.\n",
    "* Arrows represent transformations (multiplication by a matrix of weight parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Multiclass\n",
    "\n",
    "$ p(Y = y \\mid X, W) = \\frac{\\exp(XW_{y})}{ \\displaystyle\\sum_{y' \\in \\mathcal{Y}}{\\exp(XW_{y'})}} = \\textrm{softmax}(XW)_y $\n",
    "\n",
    "\n",
    "### Binary\n",
    "\n",
    "(people usually let one class be implicit and simplify)\n",
    "\n",
    "$ p(Y = 1 \\mid X, W) = \\frac{\\exp(XW)}{ \\exp(XW) + \\exp(0)} = \\frac{1}{ 1 + \\exp(-XW)} = \\textrm{sigmoid}(XW) $\n",
    "\n",
    "<img src=\"nn_diagrams_logistic.svg\" width=\"100px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categorical outputs.\n",
    "\n",
    "Subsequently, for consistency, I'll assume we have categorical outputs and use a softmax activation function.\n",
    "\n",
    "RNNs can work with other types of outputs (e.g., binary) as with linear and logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization (finding good parameter values)\n",
    "\n",
    "* Minimize the **negative log likelihood** of the parameters given the observed data (true labels).\n",
    "  * Equivalently, minimize the **cross entropy** between the distribution of the model's predicted labels and the true distribution.\n",
    "  * Or, e.g., mean squared error for linear regression with continuous data.\n",
    "* To do this, we can use algorithms based on stochastic gradient descent (e.g., Adagrad, Adam).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "$ P(Y = y \\mid X, W) = \\textrm{softmax}(ZW_{out})_y $\n",
    "\n",
    "$ Z = \\tanh(XW_{h}) $\n",
    "\n",
    "<img src=\"nn_diagrams_mlp.svg\" width=\"150px\"/>\n",
    "\n",
    "### Notes\n",
    "\n",
    "* Instead of tanh, we could use some other nonlinear function (e.g., sigmoid, ReLU).\n",
    "* For non-categorical data, we could use some output activation other than the $\\textrm{softmax}$ function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple RNN\n",
    "\n",
    "\n",
    "$ P(Y_t = y \\mid \\ldots) = \\textrm{softmax}(S_t W_{out})_y $\n",
    "\n",
    "$ S_t = f(X W_{in} + S_{t-1} W_{recur}) $\n",
    "\n",
    "\n",
    "<img src=\"nn_diagrams_simple_rnn.svg\" width=\"500px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stacked RNNs\n",
    "\n",
    "In order to (maybe) do a better job at model interactions, we can stack RNNs similar to adding hidden layers in a multilayer perceptron:\n",
    "\n",
    "\n",
    "$ P(Y_t = y \\mid \\ldots) = \\textrm{softmax}(S_{t,1} W_{out})_y $\n",
    "\n",
    "$ S_{t,1} = f(S_{t,0} W_{in,1} + S_{t-1,1} W_{recur,1}) $\n",
    "\n",
    "$ S_{t,0} = f(X W_{in,0} + S_{t-1,0} W_{recur,0}) $\n",
    "\n",
    "<img src=\"nn_diagrams_stacked_rnn.svg\" width=\"200px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning long distance dependencies\n",
    "\n",
    "* Recall that the RNN state is a combination of the previous state and new input:\n",
    "\n",
    "$ S_t = f(X W_{in} + S_{t-1} W_{recur}) $\n",
    "\n",
    "* As more input is read in, attributing information in the state to particular inputs and weights becomes difficult.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "## Gates\n",
    "\n",
    "* In Gated RNNs and Long Short Term Memory (LSTM) networks, **gates** regulate what information is retained in and added to the current state\n",
    "  * Gates depend on the previous state $s_{t-1}$ and the current input $x_t$.\n",
    "  * For example, gates could help ignore the repetition of particular information (\"very very very good\").\n",
    "\n",
    "**Related:** vanishing and exploding gradients. See, e.g., [this](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Long Short Term Memory (LSTM) recurrent networks\n",
    "\n",
    "### Simplified version\n",
    "\n",
    "Input, forget, and output gates:\n",
    "\n",
    "$ i = \\textrm{sigmoid}(x_t U^{i} + s_{t-1} W^{i}) $\n",
    "\n",
    "$ f = \\textrm{sigmoid}(x_t U^{f} + s_{t-1} W^{f}) $\n",
    "\n",
    "$ o = \\textrm{sigmoid}(x_t U^{o} + s_{t-1} W^{o}) $\n",
    "\n",
    "State update (simplified! see below):\n",
    "\n",
    "$ c_{t} = c_{t-1} \\circ f + X_t \\circ i $\n",
    "\n",
    "**Note:** \"$\\circ$\" denotes elementwise multiplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Full version\n",
    "\n",
    "Actually, LSTMs are a bit more complicated.\n",
    "\n",
    "\n",
    "\n",
    "Transformed input:\n",
    "\n",
    "$ g = \\tanh(x_t U^{g} + s_{t-1} W^{g}) $\n",
    "\n",
    "State update:\n",
    "\n",
    "$ c_{t} = c_{t-1} \\circ f + g \\circ i $\n",
    "\n",
    "Output:\n",
    "\n",
    "$ s_t = \\tanh(c_t) \\circ o $\n",
    "\n",
    "**Note:** unlike the simple RNN, the LSTM output is not just the current state's value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Variations\n",
    "\n",
    "People have tried lots of variations of the LSTM architecture.  See, e.g., [this paper](http://arxiv.org/pdf/1503.04069.pdf).\n",
    "\n",
    "Also, there are [Gated RNNs](https://arxiv.org/abs/1412.3555):\n",
    "\n",
    "* fewer parameters than LSTMs\n",
    "* often perform as well as or better than LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Embeddings for sparse data\n",
    "\n",
    "* High dimensional inputes (e.g., words) are often embedded into lower dimensions before being used as network input.\n",
    "  * e.g., 100,000 word indicators $\\rightarrow$ 300 dimensions\n",
    "  * This keeps the RNN inputs (and gates) from having very high dimensionality.\n",
    "  \n",
    "### Pretrained embeddings\n",
    "\n",
    "* Embeddings are sometimes separately pretrained on large amounts of unlabeled in-domain data.\n",
    "  * e.g., [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) or [GloVe](http://nlp.stanford.edu/projects/glove/) embeddings trained on Wikipedia.\n",
    "* This is (arguably) semi-supervised learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling scenarios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Sequence modeling\n",
    "\n",
    "* In NLP, language modeling is often used to model the probability of sequences of words (discrete variables).\n",
    "* Useful in speech recognition and machine translation, with Bayes' rule, to score potential transcriptions and translations, respectively.\n",
    "  * $p(English \\mid French) \\propto p(English) p(French \\mid English)$\n",
    "* Claude Shannon talked about this task [in the 40s](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication).\n",
    "* Applications for continuous data are also possible.\n",
    "  * e.g., other types of time series (just think of a sentence as a time series with discrete events).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Classification and regression\n",
    "\n",
    "* Use RNN output and feed it into a classifier or regressor.\n",
    "  * Take the output from last time step? From all steps (with attention or pooling to aggregate)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Transduction\n",
    "\n",
    "* We might want to produce an output for each input.\n",
    "  * e.g., POS Tagging (\"The dog ran in the park\" -> \"article noun verb preposition article noun\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Sequence to Sequence learning\n",
    "\n",
    "* We may want to perform more complex transformations from one sequence to another.\n",
    "  * e.g., machine translation, paraphrase, summarization\n",
    "* [One possible method](https://arxiv.org/pdf/1409.3215v3.pdf) (a bit outdated):\n",
    "  1. read inputs (e.g., English words) into one RNN.\n",
    "  2. take the state and feed it into a second RNN.\n",
    "  3. generate outputs (e.g., French words) from the second RNN until reaching a stop indicator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Reinforcement learning\n",
    "\n",
    "* RNNs could be useful for RL when rewards are partially observable (e.g., [this paper](https://arxiv.org/pdf/1507.06527.pdf)).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A few Ideas for Non Text Applications\n",
    "\n",
    "* modeling how media consumption patterns (e.g., TV ratings) change over time\n",
    "  * This is a bit like language modeling, but with ratings values instead of words.\n",
    "* Sensor data (#IoT)\n",
    "  * e.g., categorize signals\n",
    "  * e.g., identify patterns within a longer signal\n",
    "* Resource allocation\n",
    "  * e.g., given jobs submitted to a work queue, we could identify when to allocate more workers/resources.\n",
    "* App/website logs\n",
    "  * e.g., cluster or categorize user behavior\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some important deep learning concepts I haven't talked about\n",
    "\n",
    "* [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) or regularization to avoid overfitting\n",
    "* [initialization schemes](http://deepdish.io/2015/02/24/network-initialization/)\n",
    "* activation functions (e.g., [ReLUs](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\n",
    "* [attention](https://arxiv.org/abs/1409.0473) for sequence-to-sequence problems (and classification, etc.)\n",
    "* optimization with gradient descent (e.g., [Adam algorithm](https://arxiv.org/abs/1412.6980))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alternative methods for modeling sequences\n",
    "\n",
    "* [Probabilistic graphical models](http://homes.cs.washington.edu/~taskar/pubs/gms-srl07.pdf) over sequences\n",
    "  * undirected: [conditional random fields (CRFs)](https://en.wikipedia.org/wiki/Conditional_random_field)\n",
    "  * directed: [hidden Markov models (HMMs)](http://mlg.eng.cam.ac.uk/zoubin/papers/ijprai.pdf)\n",
    "* [discriminative Markov models](http://www.aclweb.org/anthology/W02-1001)\n",
    "  * Basically, one uses a standard classifier to predict a label for each time step using the previous prediction along with the current input.\n",
    "* The lines between these things and RNNs can be [kind of blurry](http://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More Links\n",
    "\n",
    "* [Yoav Goldberg's \"A Primer on Neural Network Models for Natural Language Processing\"](https://arxiv.org/abs/1510.00726)\n",
    "* [Neural Network Zoo](http://www.asimovinstitute.org/neural-network-zoo/)\n",
    "* [WildML's posts on RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
    "* [TensorFlow docs](https://www.tensorflow.org/versions/r0.11/tutorials/index.html)\n",
    "* [paper on Gated RNNs](https://arxiv.org/abs/1412.3555)\n",
    "* [Colah's \"Understanding LSTM Networks\"](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thanks to various folks at Civis for discussion!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
