{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To run the slideshow:\n",
    "`jupyter nbconvert heilman_pydata_chicago_2016.ipynb --to slides --post serve`\n",
    "\n",
    "To change the transition animation default, go to\n",
    "http://127.0.0.1:8000/heilman_pydata_chicago_2016.slides.html?transition=none\n",
    "\n",
    "main dependencies: TensorFlow 0.10, joblib 0.10, scikit-learn 0.17.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementing distributed grid search for deep learning using TensorFlow, scikit-learn, and joblib\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "Michael Heilman (<a target=\"_blank\" href=\"https://twitter.com/heilman13\">@heilman13</a>)\n",
    "</center>\n",
    "<center>\n",
    "Data Scientist, <a target=\"_blank\" href=\"https://civisanalytics.com/\">Civis Analytics</a> (<a target=\"_blank\" href=\"https://twitter.com/CivisAnalytics\">@CivisAnalytics</a>)\n",
    "</center>\n",
    "<center>\n",
    "2016-08-28\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is this talk about?\n",
    "\n",
    "It's about combining open source technologies to implement a machine learning workflow.\n",
    "\n",
    "1. custom machine learning\n",
    "  * TensorFlow\n",
    "2. a standard interface for machine learning\n",
    "  * scikit-learn\n",
    "3. simple distributed computing\n",
    "  * joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Custom machine learning\n",
    "\n",
    "* ML implementation is getting easier, almost like writing equations on a whiteboard.\n",
    "* **The focus here:** supervised, discriminative deep learning with gradient-based optimization\n",
    "  * **alternatives:** tree ensembles, probabilistic graphical models (PyMC3, STAN), etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Background: Supervised, Discriminative Machine Learning\n",
    "\n",
    "### Goal: find a function $f(y \\mid X, W)$ that...\n",
    "\n",
    "* has **parameters**: weights $W$\n",
    "* takes **input**: features $X$ (e.g., word counts)\n",
    "* produces **output**: labels $y$ (e.g., positive/negative sentiment)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Procedure for finding a good $f$\n",
    "\n",
    "Typically, we use **gradient-based optimization**.\n",
    "\n",
    "1. Define an objective function $g(X, y, W)$\n",
    "    * $X$, $y$: training data (fixed)\n",
    "    * Goal: find setting for $W$ that optimizes $g$.\n",
    "    * Often, $g$ is a difference between $\\hat{y} = f(y \\mid X, W)$ and $y$.\n",
    "2. Define a function to compute the derivative of $g$ with respect to $W$.\n",
    "    * This often duplicates engineering effort from implementing $f$ and $g$.\n",
    "    * For DL models, $g$ and its derivative can be **very** complicated.\n",
    "3. Iterate:\n",
    "    * Compute the derivative.\n",
    "    * Update $W$ based on the derivative.\n",
    "      * Often with complex adjustments to reduce required iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Graph-based numerical computation (e.g., TensorFlow)\n",
    "\n",
    "* **step 1** (model and objective implementation) becomes pretty easy\n",
    "* **step 2** (differentiation) becomes one line of code\n",
    "* **step 3** (iterative learning) becomes one or a few lines of code\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TensorFlow and the alternatives\n",
    "\n",
    "* TensorFlow's graph-based approach to numerical computation facilitates DL implementation.\n",
    "  * Developed by Google, now [an open source project](https://github.com/tensorflow/tensorflow).\n",
    "  * It's pretty popular: <img src=\"tf_gh_star_count.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are alternative DL tools, in Python (often with C++ underneath) and other languages.\n",
    "\n",
    "* theano (Python)\n",
    "  * very similar predecessor to TF\n",
    "  * academic research project\n",
    "* keras (Python)\n",
    "  * wraps TF and theano\n",
    "  * mainly for neural nets & deep learning\n",
    "* mxnet (Python)\n",
    "* torch (Lua)\n",
    "* just NumPy + SciPy\n",
    "  * `scipy.optimize` is handy for many things\n",
    "  * no autodiff\n",
    "* caffe \n",
    "  * pycaffe\n",
    "* Microsoft CNTK\n",
    "* others\n",
    "  * there's probably a new one as you read this slide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A brief introduction to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session 1, initial: [ 0.  0.  0.]\n",
      "session 1, after call 1: [ 1.  2.  3.]\n",
      "session 1, after call 2: [ 1.  7.  3.]\n",
      "session 1, final: [ 1. -3.  3.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is an example of accumulating a sum of 1-D tensors,\n",
    "to illustrate some major concepts in TensorFlow\n",
    "(variables, operations, graphs, and sessions).\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# A graph is a collection of tensors and operations on them.\n",
    "g = tf.Graph()\n",
    "\n",
    "# When you create tensors and operations, they are added to\n",
    "# whatever the current default graph is.\n",
    "# TF lets you use context managers rather than, e.g.,\n",
    "# having to do something like g.add(variable).\n",
    "# Note: there is also a global default graph (global scope).\n",
    "with g.as_default():\n",
    "\n",
    "    # Define a placeholder for input (which gets added to g).\n",
    "    x = tf.placeholder(dtype=np.float32, name=\"x\")\n",
    "    \n",
    "    # Make a variable, including an initializer (here just a constant value).\n",
    "    # Its value will be kept across calls to `run` within a session (more below).\n",
    "    sum_x = tf.Variable([0., 0., 0.], name=\"sum_x\")\n",
    "    \n",
    "    # You need to make sure variables are initialize in a session.\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    \n",
    "    # Make an Operation to add the input to the sum.\n",
    "    # This is also added to the graph.\n",
    "    # Note: the value returned by this op this will be `sum_x`.\n",
    "    add_x = sum_x.assign(sum_x + x)\n",
    "    \n",
    "    # Sessions are execution contexts. They capture, e.g.,\n",
    "    # the values of Variable instances in the graph.\n",
    "    # Analogy: think of a tf.Graph as like a UNIX script/program/executable\n",
    "    # and a tf.Session as like a UNIX process. It's not a perfect analogy.\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Sessions consist of multiple runs, so perhaps the run sequence\n",
    "        # is the script in the analogy above.\n",
    "        sess.run(init_op)\n",
    "        print(\"session 1, initial:\",\n",
    "              sess.run(sum_x))\n",
    "        \n",
    "        print(\"session 1, after call 1:\",\n",
    "              sess.run(add_x, feed_dict={x: [1, 2, 3]}))\n",
    "        \n",
    "        print(\"session 1, after call 2:\",\n",
    "              sess.run(add_x, feed_dict={x: [0, 5, 0]}))\n",
    "        \n",
    "        print(\"session 1, final:\",\n",
    "              sess.run(add_x, feed_dict={x: [0, -10, 0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session 2, initial sum_x: [ 0.  0.  0.]\n",
      "session 2, after call 1: [ 1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Now start another session. The previous state (variable values) is lost.\n",
    "# Note how we don't have to double indent as above.\n",
    "with g.as_default(), tf.Session() as sess:\n",
    "    # If `init_op` isn't called again, we get an exception because\n",
    "    # the variable hasn't been initialized.\n",
    "    sess.run(init_op)\n",
    "\n",
    "    print(\"session 2, initial sum_x:\", sess.run(sum_x))\n",
    "    print(\"session 2, after call 1:\",\n",
    "          sess.run(add_x, feed_dict={x: [1, 1, 1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative of y = a * x + 10.0 with respect to x:\n",
      " [[ 2.  5.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here's a quick illustration of automatic differentiation.\n",
    "Note that TensorFlow implements learning algorithms that use this internally,\n",
    "to make it easy to apply various gradient-based optimizers.\n",
    "\"\"\"\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default(), tf.Session() as sess:\n",
    "    a = tf.constant([[2.0, 5.0]])\n",
    "    x = tf.Variable([[0.0, 0.0]])\n",
    "    y = a * x + 10.0\n",
    "    y_grad = tf.gradients([y], [x])[0]  # inputs/outputs lists\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print(\"Derivative of y = a * x + 10.0 with respect to x:\\n\",\n",
    "          sess.run(y_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multilayer Perceptrons (MLPs)\n",
    "\n",
    "* a feed-forward neural network\n",
    "  * e.g., no recursion, no convolutions\n",
    "  * input layer, 1+ hidden layers, output layer\n",
    "* can model interactions between features\n",
    " * capable of approximating any function (\"universal approximation theorem\")\n",
    "* an old idea (Rosenblatt, 1961), but can be used with state-of-the-art methods\n",
    " * e.g., ReLUs, dropout, Adam learning algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img width=\"50%\" src=\"mlp_diagram.svg\"></img>\n",
    "\n",
    "$$y = f(g(x^T W_1)^T W_2$$\n",
    "$$f = \\textrm{softmax}$$\n",
    "$$g = \\textrm{sigmoid}$$\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Here, we have a softmax output function $f$, for multiclass classification.\n",
    "* ReLU hidden units could also be used, but they might be less interpretable than sigmoid hidden units.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. A standard interface for machine learning\n",
    "\n",
    "We want to follow a standard to facilitate:\n",
    "\n",
    "* use in existing scikit-learn-based workflows\n",
    "* easy comparisons to other methods in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The scikit-learn Estimator API\n",
    "\n",
    "* [scikit-learn](http://scikit-learn.org/) is a ML library developed by [many people](https://github.com/scikit-learn/scikit-learn/graphs/contributors)\n",
    "* The consistent API makes trying different ML algorithms easy.\n",
    "* It allows users to implement their own estimators.\n",
    "  * helper functions for input validation, preprocessing, testing, etc.\n",
    "  * **key feature:** `sklearn.utils.estimator_checks.check_estimator` to check API conformity of a custom estimator (free tests!)\n",
    "* It facilitates 2 big ML challenges:\n",
    "  * pipelines of transformations\n",
    "    * e.g., extract features, transform/standardize values, fit model\n",
    "  * gridsearch over hyperparameters (more on this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview for custom scikit-learn predictive models\n",
    "\n",
    "* For models, we need to implement a `fit(X, y)` and `predict(X)`\n",
    "  * optionally, also `predict_proba(X)`, etc.\n",
    "* `__init__` should just attach arguments\n",
    "  * `fit` does a lot of what `__init__` normally does\n",
    "  * `fit` usually sets instance attributes (e.g., `model.coef_`)\n",
    "* everything should be serializable with `pickle`\n",
    "  * `joblib`, used by grid search, expects things to be serializable\n",
    "* mixins for different model types\n",
    "  * `RegressorMixin`, `ClassifierMixin`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Civis MLP Implementation\n",
    "\n",
    "* follows scikit-learn's API\n",
    "* GPU support through TensorFlow\n",
    "* sparse and dense matrix support\n",
    "* generic base class + regressor and classifier subclasses\n",
    "  * classifier supports binary, multiclass, and multilabel binary classification\n",
    "* called MuFFNN (Multilayer Feed-Forward Neural Network)\n",
    "  * https://github.com/civisanalytics/muffnn\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A few alternative MLP implementations\n",
    "\n",
    "* `sklearn.neural_network.MLPClassifier` (and `MLPRegressor`)\n",
    "  * CPU-only, no GPU support\n",
    "  * may be difficult to customize (e.g., new learning algorithms)\n",
    "* `tensorflow.learn`\n",
    "  * included with TensorFlow in a `contrib` section\n",
    "  * doesn't quite follow scikit-learn interface (last I checked)\n",
    "    * e.g., `__init__` does more than attach arguments\n",
    "    * possibly difficult to use with grid search and pipelines\n",
    "  * extra support for out-of-core, minibatch learning (`DataFeeder` classes)\n",
    "* `keras.wrappers.scikit_learn.KerasClassifier`\n",
    "  * If you implement a model using keras, this can make it follow scikit-learn's API.\n",
    "  * possibly slightly less flexible than TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The MLP base class\n",
    "\n",
    "Note: I've removed some code and most documentation here for simplicity.\n",
    "\n",
    "```python\n",
    "\n",
    "class MLPBaseEstimator(BaseEstimator, metaclass=ABCMeta):\n",
    "    \n",
    "    def _preprocess_targets(self, y):\n",
    "        # Subclasses can override this to store information about the targets.\n",
    "        return y\n",
    "\n",
    "    def fit(self, X, y, monitor=None):\n",
    "        ...        \n",
    "        y = self._preprocess_targets(y)\n",
    "\n",
    "        self.graph_ = Graph()\n",
    "        with self.graph_.as_default():\n",
    "\n",
    "            # Define the model (We'll get to this in a minute).\n",
    "            self._init_model()\n",
    "\n",
    "            # Initialize weights.\n",
    "            self._session = tf.Session()\n",
    "            self._session.run(tf.initialize_all_variables())\n",
    "            ...\n",
    "\n",
    "        # Minibatch training.\n",
    "        for epoch in range(self.n_epochs):\n",
    "            random_state.shuffle(indices)\n",
    "            for start_idx in range(0, n_examples, batch_size):\n",
    "\n",
    "                # Make the dictionary assigning a minibatch of training\n",
    "                # examples (pair of arrays) to TensorFlow placeholders.\n",
    "                batch_ind = indices[start_idx:start_idx + batch_size]\n",
    "                feed_dict = self._make_feed_dict(X[batch_ind],\n",
    "                                                 y[batch_ind])\n",
    "\n",
    "                # Compute objective function and gradients and  update weights.\n",
    "                obj_val, _ = self._session.run(\n",
    "                    [self._obj_func, self._train_step],\n",
    "                    feed_dict=feed_dict)\n",
    "\n",
    "            ...\n",
    "\n",
    "        return self\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Abstract methods to be implemented by the regressor and classifier classes\n",
    "\n",
    "```python\n",
    "...\n",
    "    @abstractmethod\n",
    "    def _init_model_output(self, t):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _init_model_objective_fn(self, t):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The core MLP modeling code\n",
    "\n",
    "```python\n",
    "...\n",
    "    def _init_model(self):\n",
    "        # A placeholder variable to control dropout for training vs. prediction.\n",
    "        self._dropout = \\\n",
    "            tf.placeholder(dtype=np.float32, shape=(), name=\"dropout\")\n",
    "\n",
    "        # Input layers.\n",
    "        if self.is_sparse_:\n",
    "           ...\n",
    "        else:\n",
    "            self._input_values = \\\n",
    "                tf.placeholder(np.float32, [None, self.input_layer_sz_],\n",
    "                               \"input_values\")\n",
    "            t = self._input_values\n",
    "\n",
    "        # Hidden layers (self.hidden_units is a list of ints for HL sizes)\n",
    "        for i, layer_sz in enumerate(self.hidden_units):\n",
    "            if self.is_sparse_ ...:\n",
    "                ...\n",
    "            else:\n",
    "                t = tf.nn.dropout(t, keep_prob=self._dropout)\n",
    "                t = _affine(t, layer_sz, scope='layer_%d' % i)\n",
    "            t = t if self.activation is None else self.activation(t)\n",
    "\n",
    "        # The output layer and objective function depend on the model.\n",
    "        t = self._init_model_output(t)\n",
    "        self._init_model_objective_fn(t)\n",
    "\n",
    "        # Set the training algorithm, which is currently not configurable.\n",
    "        self._train_step = tf.train.AdamOptimizer().minimize(self._obj_func)\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can't pickle a tf.Graph\n",
      "You can't pickle a tf.Session.\n"
     ]
    }
   ],
   "source": [
    "# You can't pickle some TensorFlow objects, at least as of version 0.10.0rc0.\n",
    "\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    g = tf.Graph()\n",
    "    pickle.dumps(g)\n",
    "except TypeError:\n",
    "    print(\"You can't pickle a tf.Graph\")\n",
    "\n",
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        pickle.dumps(sess)\n",
    "except TypeError:\n",
    "    print(\"You can't pickle a tf.Session.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Methods to support pickling\n",
    "\n",
    "* TensorFlow has a `Saver` class that writes models and parameters to disk.\n",
    "* One can override `__getstate__`, the method `pickle` uses to get an instance's data.\n",
    "* A not-so-elegent workaround to support TF pickling:\n",
    "  * write the model to disk\n",
    "  * read it in as bytes\n",
    "  * pickle\n",
    "* unpickling is the reverse\n",
    "\n",
    "```python\n",
    "...\n",
    "   # Used when saving:\n",
    "   def __getstate__(self):\n",
    "        # Write out the model.\n",
    "        ...\n",
    "        if getattr(self, '_fitted', False):\n",
    "            tempfile = NamedTemporaryFile(delete=False)\n",
    "            tempfile.close()\n",
    "            try:\n",
    "                # Serialize the model and read it so it can be pickled.\n",
    "                self._saver.save(self._session, tempfile.name)\n",
    "                with open(tempfile.name, 'rb') as f:\n",
    "                    saved_model = f.read()\n",
    "            finally:\n",
    "                os.unlink(tempfile.name)\n",
    "        ...\n",
    "\n",
    "        # Note: don't include the graph since it can be recreated.\n",
    "        state = dict(\n",
    "            activation=self.activation,\n",
    "            batch_size=self.batch_size,\n",
    "            ...\n",
    "        )\n",
    "\n",
    "        # Add fitted attributes if the model has been fit.\n",
    "        if getattr(self, '_fitted', False):\n",
    "            state['_fitted'] = True\n",
    "            state['input_layer_sz_'] = self.input_layer_sz_\n",
    "            state['is_sparse_'] = self.is_sparse_\n",
    "            state['_saved_model'] = saved_model\n",
    "        ...\n",
    "\n",
    "        # Return what can and should be pickled.\n",
    "        return state\n",
    "\n",
    "    # Used when loading:\n",
    "    def __setstate__(self, state):\n",
    "        \n",
    "        # Set hyperparameters, which pickled in the usual way.\n",
    "        for k, v in state.items():\n",
    "            if k in ['saved_model']:\n",
    "                continue\n",
    "            self.__dict__[k] = v\n",
    "\n",
    "        # Reinitialize a Graph and Session, and restore the saved values.\n",
    "        ...\n",
    "        if state['_saved_model'] is not None:\n",
    "            tempfile = NamedTemporaryFile(delete=False)\n",
    "            tempfile.close()\n",
    "            try:\n",
    "                # Write out the serialized model that can be restored by TF.\n",
    "                with open(tempfile.name, 'wb') as f:\n",
    "                    f.write(state['_saved_model'])\n",
    "                self.graph_ = Graph()\n",
    "                with self.graph_.as_default():\n",
    "                    self._init_model()\n",
    "                    self._session = tf.Session()\n",
    "                    self._saver.restore(self._session, tempfile.name)\n",
    "            finally:\n",
    "                os.unlink(tempfile.name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classifier subclass\n",
    "\n",
    "```python\n",
    "class MLPClassifier(MLPBaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_units=(256,), batch_size=64, n_epochs=5,\n",
    "                 dropout=None, activation=nn.relu, init_scale=0.1,\n",
    "                 random_state=None):\n",
    "        self.hidden_units = hidden_units\n",
    "        self.batch_size = batch_size\n",
    "        ...\n",
    "\n",
    "    def _init_model_output(self, t):\n",
    "        # Determine the output layer size.\n",
    "        if self.multilabel_:\n",
    "            ...\n",
    "        elif self.n_classes_ > 2:\n",
    "            ...\n",
    "        else:\n",
    "            # Binary classification\n",
    "            output_size = 1\n",
    "\n",
    "        # Add the final affine transformation.\n",
    "        if self.is_sparse_ ...:\n",
    "            ...\n",
    "        else:\n",
    "            t = tf.nn.dropout(t, keep_prob=self._dropout)\n",
    "            t = _affine(t, output_size, scope='output_layer')\n",
    "\n",
    "        # Add the output layer activation function.\n",
    "        if self.multilabel_:\n",
    "            ...\n",
    "        elif self.n_classes_ > 2:\n",
    "            ...\n",
    "        else:\n",
    "            # Binary classification\n",
    "            self.input_targets_ = tf.placeholder(tf.int64, [None], \"targets\")\n",
    "            t = tf.reshape(t, [-1])  # Convert to 1d tensor.\n",
    "            self.output_layer_ = tf.nn.sigmoid(t)\n",
    "        return t\n",
    "\n",
    "    def _init_model_objective_fn(self, t):\n",
    "        if self.multilabel_:\n",
    "            ...\n",
    "        elif self.n_classes_ > 2:\n",
    "            ...\n",
    "        else:\n",
    "            # Binary classification\n",
    "            cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                t, tf.cast(self.input_targets_, np.float32))\n",
    "        self._obj_func = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    def _preprocess_targets(self, y):\n",
    "        # Store a mapping between class label (e.g., strings) and indices.\n",
    "        ...\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        ...\n",
    "\n",
    "    def predict(self, X):\n",
    "        ...\n",
    "\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Simple distributed computing\n",
    "\n",
    "* **motivation:** ML methods have lots of hyperparameters.\n",
    "  * **deep learning:** numbers and sizes of hidden layers, etc.\n",
    "  * **probabilistic graphical models:** parameters for priors\n",
    "  * **tree ensembles:** depth, learning rate, samples per split, etc.\n",
    "\n",
    "* grid search\n",
    "  * try a lots of settings, pick the one leading to the best performance on held-out development data\n",
    "  * often combined with $k$-fold cross-validation (e.g., scikit-learn's `GridSearchCV`)\n",
    "  * crude but **embarassingly parallel**\n",
    " \n",
    "* Alternative hyperparameter search methods exist.\n",
    "  * randomized search, HyperOpt, BayesOpt, etc.\n",
    "  * also parallelizable to some extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error reduction (k-fold CV): 14%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A realllllly simple illustration of how hyperparameter tuning matters.\n",
    "\n",
    "Tuning the regularization for logistic regression on the \"digits\"\n",
    "dataset in scikit-learn leads to a 14% error reduction in k-fold CV.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "digits = load_digits()\n",
    "est = LogisticRegression(random_state=1234)\n",
    "gs = GridSearchCV(est, param_grid={\"C\": [10.0 ** x for x in range(-5, 6)]})\n",
    "gs.fit(digits.data, digits.target)\n",
    "df = pd.DataFrame({'CV accuracy': [x.mean_validation_score\n",
    "                                   for x in gs.grid_scores_],\n",
    "                   'C': [x.parameters['C'] for x in gs.grid_scores_]})\n",
    "df.plot(x='C', y='CV accuracy', logx=True, figsize=(6, 4))\n",
    "plt.savefig('hyperparameter_search.png')\n",
    "\n",
    "default_score = gs.grid_scores_[5].mean_validation_score\n",
    "print(\"error reduction (k-fold CV): {0:.0f}%\"\n",
    "      .format(100 * (gs.best_score_ - default_score) / (1 - default_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"hyperparameter_search.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## joblib\n",
    "\n",
    "* a Python package for running embarassingly parallel tasks\n",
    "  * contributors: [@GaelVaroquaux](https://twitter.com/GaelVaroquaux), [@ogrisel](https://twitter.com/ogrisel), *et al.*\n",
    "* originally focused on single machine parallelization\n",
    "  * backends for multiprocessing, multithreading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A simple joblib example\n",
    "\n",
    "1. Select a backend (or use the default multiprocessing backend).\n",
    "2. Instantiate a `Parallel` callable.\n",
    "3. Call it on a bunch of `delayed` function calls and their arguments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map a function f(x)=10*x**2 over a list of numbers:\n",
      " [0, 10, 40, 90, 160, 250, 360, 490, 640, 810]\n"
     ]
    }
   ],
   "source": [
    "from joblib.parallel import delayed, parallel_backend, Parallel\n",
    "\n",
    "def foo(x):\n",
    "    return 10 * x ** 2\n",
    "\n",
    "with parallel_backend(\"multiprocessing\"):\n",
    "    parallel = Parallel()\n",
    "    numbers = range(10)\n",
    "    print(\"map a function f(x)=10*x**2 over a list of numbers:\\n\",\n",
    "          parallel(delayed(foo)(x) for x in numbers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### joblib in scikit-learn\n",
    "\n",
    "* scikit-learn uses joblib to [parallelize hyperparameter evaluation](https://github.com/scikit-learn/scikit-learn/blob/668b329d421e797d0b5dbea9035c5de986da60a5/sklearn/grid_search.py#L552-L561) in `GridSearchCV`\n",
    "* If you tell joblib to use a custom backend, scikit-learn will use that for grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## joblib custom backends\n",
    "\n",
    "* version 0.10.0 supports custom parallel backends\n",
    "  * example: [dask distributed backend](https://github.com/dask/distributed/blob/master/docs/source/joblib.rst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### implementing a custom backend\n",
    "\n",
    "* a class representing a pending result\n",
    "  * similar to a `concurrent.Future`\n",
    "  * `get` method to block and retrieve the result\n",
    "  * `result` attribute to cache the result\n",
    "* a `joblib._parallel_backends.ParallelBackendBase` subclass\n",
    "  * `effective_n_jobs` method that determines how many jobs can run in parallel\n",
    "    * e.g., can be used to cap the number of cores used across a cluster\n",
    "  * `apply_async` method\n",
    "    * queues up computation of a function call\n",
    "    * returns a result instance (see above)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Civis's computation infrastructure\n",
    "\n",
    "* Civis has an autoscaling EC2- and Docker-based system for running jobs.\n",
    "  * alternatives: kubernetes, mesos\n",
    "* Jobs are independent (currently, at least).\n",
    "* We have a Python client with a `concurrent.futures.Executor` interface.\n",
    "  * This allows us to submit docker run commands to remote workers.\n",
    "  * We'll use a joblib backend to facilitate getting functions and data to the workers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline of our backend implementation\n",
    "\n",
    "```python\n",
    "\n",
    "class _CivisBackend(ParallelBackendBase):\n",
    "    def __init__(self, ...):\n",
    "        ...\n",
    " \n",
    "        # Initialize an executor for making and running Docker containers.\n",
    "        self.executor = ContainerPoolExecutor(**executor_kwargs)\n",
    "        ...\n",
    "\n",
    "    def effective_n_jobs(self, n_jobs):\n",
    "        # e.g., set a hard limit on the number of jobs.\n",
    "        ...\n",
    "\n",
    "    def apply_async(self, func, callback=None):\n",
    "        ...\n",
    "        \n",
    "        with TemporaryDirectory() as tempdir:\n",
    "            # Serialize func to a temporary file and upload it to Civis.\n",
    "            ...\n",
    "            \n",
    "            # Make a command for the remote Docker container that will run\n",
    "            # a script that will download the serialized function `func`,\n",
    "            # run it, and store the result.\n",
    "            cmd = (...\n",
    "                   \"python {runner_script} {func_file_id}\"\n",
    "                   ...)\n",
    "            ...\n",
    "            \n",
    "            # Submit the command to be executed in a remote Docker container.\n",
    "            future = self.executor.submit(cmd)\n",
    "            ...\n",
    "\n",
    "            # Wait for the job to finish.\n",
    "            result = _CivisFutureResult(future, callback)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class _CivisFutureResult:\n",
    "    def __init__(self, future, callback):\n",
    "        ...\n",
    "\n",
    "    def get(self):\n",
    "        if self.result is None:\n",
    "            ...\n",
    "\n",
    "            # Wait for the script to complete.\n",
    "            self._future.result()\n",
    "\n",
    "            ...\n",
    "\n",
    "            # Download and deserialize the result.\n",
    "            with TemporaryDirectory() as tempdir:\n",
    "                temppath = os.path.join(tempdir, \"civis_joblib_backend_result\")\n",
    "\n",
    "                # Download the serialized result.\n",
    "                ...\n",
    "                self.result = joblib.load(temppath)\n",
    "            ...\n",
    "\n",
    "        return self.result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Does it work?\n",
    "\n",
    "1. Evaluate the MLP vs. logistic regression in terms of model accuracy.\n",
    "2. Evaluate grid searching on my laptop versus distributed in the #cloud in terms of wall time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## A basic evaluation for text classification\n",
    "\n",
    "* Pang & Lee (ACL 2004) Movie Reviews dataset\n",
    "  * 2,000 movie reviews with positive/negative (0/1) sentiment polarity labels\n",
    "* features\n",
    "  * character $n$-grams from sklearn's `CountVectorizer`\n",
    "* Evaluation setup:\n",
    "  * randomly split 75% for training, 25% for testing\n",
    "  * hyperparameter grid search on the training set\n",
    "  * evaluate with the ROC AUC score of testing set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import get_scorer\n",
    "from muffnn import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/civisemployee/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load the movie reviews data\n",
    "# (polarity dataset v2.0 here: http://www.cs.cornell.edu/people/pabo/movie-review-data/).\n",
    "nltk.download('movie_reviews')\n",
    "X = np.array([movie_reviews.raw(i) for i in movie_reviews.fileids()])\n",
    "y = np.array([1 if x.split('/')[0] == 'pos' else 0\n",
    "              for x in movie_reviews.fileids()])\n",
    "splits = ShuffleSplit(X.shape[0],  n_iter=1, test_size=0.25, random_state=1234)\n",
    "train_ind, test_ind = [x for x in splits][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# character n-grams and a Multilayer Perceptron classifier\n",
    "ct_vect = CountVectorizer(analyzer='char', ngram_range=(2, 5),\n",
    "                          max_features=50000)\n",
    "mlp = MLPClassifier(n_epochs=10, random_state=42)\n",
    "pipeline = Pipeline(steps=[('char_ngram', ct_vect),\n",
    "                           ('mlp', mlp)])\n",
    "param_grid = {\n",
    "    'mlp__hidden_units': [(512,), (256,), (256, 128, 64)],\n",
    "    'mlp__dropout': [None, 0.5]\n",
    "}\n",
    "gs_mlp = GridSearchCV(pipeline, param_grid=param_grid,\n",
    "                      n_jobs=4, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# baseline: character n-grams and a logistic regression classifier\n",
    "ct_vect = CountVectorizer(analyzer='char', ngram_range=(2, 5),\n",
    "                          max_features=50000)\n",
    "lr = LogisticRegression(random_state=42)\n",
    "pipeline = Pipeline(steps=[('char_ngram', ct_vect),\n",
    "                           ('lr', lr)])\n",
    "param_grid = {\n",
    "   'lr__C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "}\n",
    "gs_lr = GridSearchCV(pipeline, param_grid=param_grid,\n",
    "                     n_jobs=4, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%time gs_lr.fit(X[train_ind], y[train_ind])\n",
    "%time gs_mlp.fit(X[train_ind], y[train_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"laptop_cpu_screenshot.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "CPU times: user 27.5 s, sys: 876 ms, total: 28.3 s\n",
    "Wall time: 4min\n",
    "CPU times: user 9min 1s, sys: 4min 54s, total: 13min 55s\n",
    "Wall time: 51min 32s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "roc_auc_scorer = get_scorer('roc_auc')\n",
    "print(\"Logistic Regression ROC AUC: {:.4f}\".format(\n",
    "      roc_auc_scorer(gs_lr, X[test_ind], y[test_ind])))\n",
    "print(\"MLP ROC AUC: {:.4f}\".format(\n",
    "      roc_auc_scorer(gs_mlp, X[test_ind], y[test_ind])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "Logistic Regression ROC AUC: 0.9119\n",
    "MLP ROC AUC: 0.9271\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## With distributed grid search\n",
    "\n",
    "```python\n",
    "factory = make_backend_factory(\n",
    "    required_resources={\"cpu\": 2048, \"memory\": 4096}, ...)\n",
    "register_parallel_backend('civis', factory)\n",
    "\n",
    "with parallel_backend('civis'):\n",
    "    gs_mlp.fit(X[train_ind], y[train_ind])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wall time:\n",
    "* laptop: 51min 32s\n",
    "* distributed: 16min 23s\n",
    "\n",
    "Notes: both include refitting on all the data locally after grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parting thoughts (speculation)\n",
    "\n",
    "* With deep learning, speeding up hyperparameter search is important.\n",
    "* Distributed grid search > distributed model fitting\n",
    "  * (probably, currently, for many applications)\n",
    "* When combining open-source tools, `whole > np.sum(parts)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thanks\n",
    "\n",
    "* Matt Becker, Bill Lattner, Derrick Higgins (for code review)\n",
    "* Walt Askew (for code contributions)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
